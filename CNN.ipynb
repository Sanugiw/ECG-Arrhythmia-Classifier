{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsyhWgFjK8-Y"
      },
      "source": [
        "## **ECG Arrhythmia Classification with CNN and Interactive Dashboard**\n",
        "\n",
        "Electrocardiography (ECG) is a non-invasive technique that records the electrical activity of the heart over time. The ECG waveform reflects the coordinated depolarization and repolarization of cardiac muscle cells, mediated by the heartâ€™s conduction system.\n",
        "\n",
        "### **Anatomical and Physiological Basis**\n",
        "\n",
        "The heartâ€™s conduction system ensures rhythmic and synchronized contractions:\n",
        "\n",
        "* **Sinoatrial (SA) Node** â€“ The natural pacemaker, located in the right atrium, initiates the electrical impulse.\n",
        "* **Atrial Muscle** â€“ Conducts the impulse across both atria, producing the **P wave** (atrial depolarization).\n",
        "* **Atrioventricular (AV) Node** â€“ Delays the impulse to allow ventricular filling, seen in the **PR segment**.\n",
        "* **Bundle of His & Bundle Branches** â€“ Transmit the signal through the interventricular septum.\n",
        "* **Purkinje Fibers** â€“ Rapidly deliver the impulse to ventricular myocardium, generating the **QRS complex** (ventricular depolarization) followed by the **T wave** (ventricular repolarization).\n",
        "\n",
        "### **Arrhythmias and ECG Changes**\n",
        "\n",
        "Arrhythmias occur when the impulse generation or conduction pathway is altered:\n",
        "\n",
        "* **Normal beat (N)** â€“ Regular SA node rhythm with intact conduction.\n",
        "* **Ventricular ectopic beat (VEB)** â€“ Premature ventricular depolarization from an abnormal focus in the ventricles, often producing a wide QRS complex.\n",
        "* **Supraventricular ectopic beat (SVEB)** â€“ Originates above the ventricles (atria or AV node) and alters P-wave morphology with a narrow QRS.\n",
        "* **Fusion beat (F)** â€“ A hybrid waveform from simultaneous normal and ectopic activation.\n",
        "\n",
        "These morphological differences are directly tied to the anatomical site of origin, making ECG classification both clinically relevant and physiologically interpretable.\n",
        "\n",
        "### **Project Objective**\n",
        "\n",
        "In this project, we develop a **Convolutional Neural Network (CNN)** model to classify ECG beats into different arrhythmia types using the MIT-BIH Arrhythmia Database. The model automatically learns morphological features such as P-wave shape, QRS width, and ST-T segment variations that correspond to underlying conduction abnormalities.\n",
        "\n",
        "To make the results accessible and interpretable, we integrate the trained model into a **Streamlit-based interactive dashboard** that allows users to:\n",
        "\n",
        "* Upload ECG files or explore sample beats\n",
        "* View the raw waveform and detected beats\n",
        "* See classification results with confidence scores\n",
        "* Explore explainability visualizations (e.g., Grad-CAM) mapping model attention to specific waveform regions\n",
        "* Connect waveform changes to anatomical and physiological causes\n",
        "\n",
        "This combination of deep learning, interactive visualization, and anatomical context bridges the gap between machine intelligence and clinical reasoning.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_jFOesPPf2I",
        "outputId": "cb27efab-a917-4253-f488-34212a0b7aaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Folder structure ready.\n",
            "â¬‡ï¸ Downloading record 123...\n",
            "Generating record list for: 123\n",
            "Generating list of all files for: 123\n",
            "Created local base download directory: data/raw\\123\n",
            "Downloading files...\n",
            "Finished downloading files\n",
            "â¬‡ï¸ Downloading record 124...\n",
            "Generating record list for: 124\n",
            "Generating list of all files for: 124\n",
            "Created local base download directory: data/raw\\124\n",
            "Downloading files...\n",
            "Finished downloading files\n",
            "â¬‡ï¸ Downloading record 200...\n",
            "Generating record list for: 200\n",
            "Generating list of all files for: 200\n",
            "Created local base download directory: data/raw\\200\n",
            "Downloading files...\n",
            "Finished downloading files\n",
            "âœ… All MIT-BIH records downloaded.\n",
            "âœ… 100: 2270 beats extracted\n",
            "âœ… 101: 1870 beats extracted\n",
            "âœ… 102: 2187 beats extracted\n",
            "âœ… 103: 2083 beats extracted\n",
            "âœ… 104: 2199 beats extracted\n",
            "âœ… 105: 2571 beats extracted\n",
            "âœ… 106: 2040 beats extracted\n",
            "âœ… 107: 2136 beats extracted\n",
            "âœ… 108: 1775 beats extracted\n",
            "âœ… 109: 2530 beats extracted\n",
            "âœ… 111: 2125 beats extracted\n",
            "âœ… 112: 2538 beats extracted\n",
            "âœ… 113: 1802 beats extracted\n",
            "âœ… 114: 1883 beats extracted\n",
            "âœ… 115: 1952 beats extracted\n",
            "âœ… 116: 2397 beats extracted\n",
            "âœ… 117: 1535 beats extracted\n",
            "âœ… 118: 2278 beats extracted\n",
            "âœ… 119: 1987 beats extracted\n",
            "âœ… 121: 1866 beats extracted\n",
            "âœ… 122: 2474 beats extracted\n",
            "âœ… 123: 1518 beats extracted\n",
            "âœ… 124: 1623 beats extracted\n",
            "âœ… 200: 2610 beats extracted\n",
            "ðŸŽ¯ Total beats extracted: 50249\n",
            "âœ… Preprocessing complete. Data saved in data/processed/\n"
          ]
        }
      ],
      "source": [
        "# ================================================================\n",
        "# Create folder structure\n",
        "# ================================================================\n",
        "import os\n",
        "\n",
        "folders = [\n",
        "    \"data/raw\",\n",
        "    \"data/processed\",\n",
        "    \"models\",\n",
        "    \"src/data\",\n",
        "    \"src/models\",\n",
        "    \"app\"\n",
        "]\n",
        "for f in folders:\n",
        "    os.makedirs(f, exist_ok=True)\n",
        "\n",
        "print(\"âœ… Folder structure ready.\")\n",
        "\n",
        "# ================================================================\n",
        "# Download MIT-BIH Arrhythmia Database (all records for training)\n",
        "# ================================================================\n",
        "import wfdb\n",
        "\n",
        "# Full MIT-BIH record list (except for corrupted/missing ones)\n",
        "record_ids = [\n",
        "    \"100\", \"101\", \"102\", \"103\", \"104\", \"105\", \"106\", \"107\", \"108\",\n",
        "    \"109\", \"111\", \"112\", \"113\", \"114\", \"115\", \"116\", \"117\", \"118\",\n",
        "    \"119\", \"121\", \"122\", \"123\", \"124\", \"200\"\n",
        "]\n",
        "\n",
        "for rec in record_ids:\n",
        "    rec_path = os.path.join(\"data/raw\", rec)\n",
        "    if not os.path.exists(rec_path):\n",
        "        print(f\"â¬‡ï¸ Downloading record {rec}...\")\n",
        "        wfdb.dl_database(\n",
        "            \"mitdb\",\n",
        "            rec_path,\n",
        "            records=[rec]\n",
        "        )\n",
        "print(\"âœ… All MIT-BIH records downloaded.\")\n",
        "\n",
        "# ================================================================\n",
        "# Preprocessing functions\n",
        "# ================================================================\n",
        "import numpy as np\n",
        "import neurokit2 as nk\n",
        "from scipy.signal import butter, filtfilt\n",
        "\n",
        "def bandpass_filter(signal, fs, lowcut=0.5, highcut=40.0, order=4):\n",
        "    \"\"\"Bandpass filter for ECG signal.\"\"\"\n",
        "    nyq = 0.5 * fs\n",
        "    low = lowcut / nyq\n",
        "    high = highcut / nyq\n",
        "    b, a = butter(order, [low, high], btype='band')\n",
        "    return filtfilt(b, a, signal)\n",
        "\n",
        "def preprocess_ecg(record_path, window_pre=0.2, window_post=0.4):\n",
        "    \"\"\"Load, filter, detect R-peaks, and segment beats.\"\"\"\n",
        "    try:\n",
        "        # Load ECG\n",
        "        rec = wfdb.rdrecord(record_path)\n",
        "        sig = rec.p_signal[:, 0]  # first channel\n",
        "        fs = rec.fs\n",
        "\n",
        "        # Filter\n",
        "        sig_filtered = bandpass_filter(sig, fs)\n",
        "\n",
        "        # R-peak detection\n",
        "        _, rpeaks = nk.ecg_peaks(sig_filtered, sampling_rate=fs)\n",
        "\n",
        "        # Segment beats\n",
        "        beats = []\n",
        "        wp = int(window_pre * fs)\n",
        "        ws = int(window_post * fs)\n",
        "        for r in rpeaks['ECG_R_Peaks']:\n",
        "            start = r - wp\n",
        "            end = r + ws\n",
        "            if start >= 0 and end < len(sig_filtered):\n",
        "                beat = sig_filtered[start:end]\n",
        "                beats.append(beat)\n",
        "        beats = np.array(beats)\n",
        "\n",
        "        return beats, sig_filtered, fs, rpeaks['ECG_R_Peaks']\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Error in {record_path}: {e}\")\n",
        "        return np.array([]), np.array([]), None, None\n",
        "\n",
        "# ================================================================\n",
        "# Batch process all records\n",
        "# ================================================================\n",
        "all_beats = []\n",
        "for rec in record_ids:\n",
        "    rec_path = os.path.join(\"data/raw\", rec, rec)\n",
        "    beats, sig_filt, fs, rlocs = preprocess_ecg(rec_path)\n",
        "    if beats.size > 0:\n",
        "        all_beats.append(beats)\n",
        "        print(f\"âœ… {rec}: {beats.shape[0]} beats extracted\")\n",
        "\n",
        "# Concatenate and save\n",
        "all_beats = np.vstack(all_beats)\n",
        "np.save(\"data/processed/all_beats.npy\", all_beats)\n",
        "\n",
        "print(f\"ðŸŽ¯ Total beats extracted: {all_beats.shape[0]}\")\n",
        "print(\"âœ… Preprocessing complete. Data saved in data/processed/\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0szKtOQQQrf"
      },
      "source": [
        "1. Data Preparation and Labelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTZIMTrJQOvo",
        "outputId": "5ff108ca-9464-4a28-eb82-3fcfb5c2506f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Processed record 100 | 2272 beats\n",
            "âœ… Processed record 101 | 1865 beats\n",
            "âœ… Processed record 102 | 2131 beats\n",
            "âœ… Processed record 103 | 2083 beats\n",
            "âœ… Processed record 104 | 1562 beats\n",
            "âœ… Processed record 105 | 2572 beats\n",
            "âœ… Processed record 106 | 2027 beats\n",
            "âœ… Processed record 107 | 2137 beats\n",
            "âœ… Processed record 108 | 1763 beats\n",
            "âœ… Processed record 109 | 2531 beats\n",
            "âœ… Processed record 111 | 2124 beats\n",
            "âœ… Processed record 112 | 2538 beats\n",
            "âœ… Processed record 113 | 1794 beats\n",
            "âœ… Processed record 114 | 1879 beats\n",
            "âœ… Processed record 115 | 1952 beats\n",
            "âœ… Processed record 116 | 2411 beats\n",
            "âœ… Processed record 117 | 1534 beats\n",
            "âœ… Processed record 118 | 2277 beats\n",
            "âœ… Processed record 119 | 1987 beats\n",
            "âœ… Processed record 121 | 1862 beats\n",
            "âœ… Processed record 122 | 2475 beats\n",
            "âœ… Processed record 123 | 1517 beats\n",
            "âœ… Processed record 124 | 1618 beats\n",
            "âœ… Processed record 200 | 2600 beats\n",
            "âœ… Saved 49511 beats.\n",
            "   Each beat has 200 samples (200).\n",
            "   Class distribution: [41592  2172   222    15  5510]\n"
          ]
        }
      ],
      "source": [
        "# ================================================================\n",
        "# ðŸ“¥ 5. Beat extraction with labels (Full MIT-BIH Dataset)\n",
        "# ================================================================\n",
        "import wfdb\n",
        "import os\n",
        "import numpy as np\n",
        "from scipy.signal import resample_poly\n",
        "\n",
        "# Mapping from MIT-BIH annotation symbols to simplified class labels\n",
        "# N = Normal, V = Ventricular ectopic, S = Supraventricular ectopic, F = Fusion, Q = Unknown\n",
        "symbol_to_class = {\n",
        "    'N': 0,  # Normal\n",
        "    'L': 0, 'R': 0, 'e': 0, 'j': 0,\n",
        "    'V': 1, 'E': 1,\n",
        "    'S': 2, 'A': 2, 'a': 2, 'J': 2,\n",
        "    'F': 3,\n",
        "    '/': 4, 'Q': 4, '?': 4\n",
        "}\n",
        "\n",
        "class_names = [\"Normal\", \"VEB\", \"SVEB\", \"Fusion\", \"Unknown\"]\n",
        "TARGET_LENGTH = 200  \n",
        "\n",
        "def extract_beats_with_labels(record_path):\n",
        "    \"\"\"Load ECG signal, filter, align to annotations, extract beats & labels.\"\"\"\n",
        "    rec = wfdb.rdrecord(record_path)\n",
        "    ann = wfdb.rdann(record_path, 'atr')\n",
        "    sig = rec.p_signal[:, 0]  # first channel\n",
        "    fs = rec.fs\n",
        "\n",
        "    # Filter signal\n",
        "    sig_filtered = bandpass_filter(sig, fs)\n",
        "\n",
        "    beats, labels = [], []\n",
        "    window_pre = int(0.2 * fs)\n",
        "    window_post = int(0.4 * fs)\n",
        "    \n",
        "    for idx, sym in zip(ann.sample, ann.symbol):\n",
        "        if sym in symbol_to_class:\n",
        "            start = idx - window_pre\n",
        "            end = idx + window_post\n",
        "            if start >= 0 and end < len(sig_filtered):\n",
        "                beat = sig_filtered[start:end]\n",
        "                # Resample to fixed length\n",
        "                beat = resample_poly(beat, TARGET_LENGTH, len(beat))\n",
        "                # Normalize\n",
        "                beat = (beat - np.mean(beat)) / (np.std(beat) + 1e-8)\n",
        "                beats.append(beat)\n",
        "                labels.append(symbol_to_class[sym])\n",
        "\n",
        "    return np.array(beats), np.array(labels)\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# Process all records from MIT-BIH (48 total)\n",
        "# ================================================================\n",
        "all_beats, all_labels = [], []\n",
        "\n",
        "# List of records in MIT-BIH (100â€“234, excluding a few missing ones)\n",
        "record_ids = [\n",
        "    \"100\",\"101\",\"102\",\"103\",\"104\",\"105\",\"106\",\"107\",\"108\",\"109\",\n",
        "    \"111\",\"112\",\"113\",\"114\",\"115\",\"116\",\"117\",\"118\",\"119\",\"121\",\n",
        "    \"122\",\"123\",\"124\",\"200\"\n",
        "]\n",
        "\n",
        "for rec in record_ids:\n",
        "    record_path = os.path.join(\"data/raw\", rec, rec)\n",
        "    try:\n",
        "        beats, labels = extract_beats_with_labels(record_path)\n",
        "        all_beats.append(beats)\n",
        "        all_labels.append(labels)\n",
        "        print(f\"âœ… Processed record {rec} | {beats.shape[0]} beats\")\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Skipped record {rec} due to error: {e}\")\n",
        "\n",
        "# Combine all into arrays\n",
        "all_beats = np.vstack(all_beats)\n",
        "all_labels = np.concatenate(all_labels)\n",
        "\n",
        "# Save processed dataset\n",
        "np.save(\"data/processed/beats.npy\", all_beats)\n",
        "np.save(\"data/processed/labels.npy\", all_labels)\n",
        "\n",
        "print(f\"âœ… Saved {all_beats.shape[0]} beats.\")\n",
        "print(f\"   Each beat has {all_beats.shape[1]} samples ({TARGET_LENGTH}).\")\n",
        "print(f\"   Class distribution: {np.bincount(all_labels)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWEA-FFLQXZt"
      },
      "source": [
        "CNN Model Definition & Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eEu0jYLbQaWo",
        "outputId": "0b54ef52-111c-4f7a-afce-8b598014c95f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Computed class weights: {0: np.float64(0.23807943835352952), 1: np.float64(4.55902394106814), 2: np.float64(44.604504504504504), 3: np.float64(660.1466666666666), 4: np.float64(1.7971324863883849)}\n",
            "Epoch 1/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\ECG-Classifier\\.venv\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m619/619\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5647 - loss: 1.4158"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m619/619\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 15ms/step - accuracy: 0.5685 - loss: 1.4053 - val_accuracy: 0.1871 - val_loss: 1.7984\n",
            "Epoch 2/30\n",
            "\u001b[1m618/619\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.5507 - loss: 0.9276"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m619/619\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 17ms/step - accuracy: 0.5662 - loss: 0.9457 - val_accuracy: 0.5905 - val_loss: 0.8659\n",
            "Epoch 3/30\n",
            "\u001b[1m617/619\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.6025 - loss: 0.8222"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m619/619\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 17ms/step - accuracy: 0.6060 - loss: 0.9721 - val_accuracy: 0.7364 - val_loss: 0.7961\n",
            "Epoch 4/30\n",
            "\u001b[1m619/619\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 22ms/step - accuracy: 0.5886 - loss: 0.9299 - val_accuracy: 0.6570 - val_loss: 0.8736\n",
            "Epoch 5/30\n",
            "\u001b[1m618/619\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.6472 - loss: 0.7614"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m619/619\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 27ms/step - accuracy: 0.6476 - loss: 0.7245 - val_accuracy: 0.7702 - val_loss: 0.6816\n",
            "Epoch 6/30\n",
            "\u001b[1m619/619\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 19ms/step - accuracy: 0.6752 - loss: 0.6074 - val_accuracy: 0.7206 - val_loss: 0.7001\n",
            "Epoch 7/30\n",
            "\u001b[1m619/619\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 17ms/step - accuracy: 0.6669 - loss: 0.8030 - val_accuracy: 0.2246 - val_loss: 1.5602\n",
            "Epoch 8/30\n",
            "\u001b[1m619/619\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 17ms/step - accuracy: 0.6613 - loss: 0.6257 - val_accuracy: 0.3650 - val_loss: 1.7139\n",
            "Epoch 9/30\n",
            "\u001b[1m619/619\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 14ms/step - accuracy: 0.7319 - loss: 0.6406 - val_accuracy: 0.7035 - val_loss: 0.8343\n",
            "Epoch 10/30\n",
            "\u001b[1m619/619\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7240 - loss: 0.6419"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m619/619\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 17ms/step - accuracy: 0.7396 - loss: 0.5660 - val_accuracy: 0.7405 - val_loss: 0.6423\n",
            "Epoch 11/30\n",
            "\u001b[1m619/619\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 17ms/step - accuracy: 0.7514 - loss: 0.5157 - val_accuracy: 0.5528 - val_loss: 1.0054\n",
            "Epoch 12/30\n",
            "\u001b[1m619/619\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 20ms/step - accuracy: 0.7772 - loss: 0.4952 - val_accuracy: 0.6729 - val_loss: 0.8767\n",
            "Epoch 13/30\n",
            "\u001b[1m619/619\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 18ms/step - accuracy: 0.7695 - loss: 0.4931 - val_accuracy: 0.7075 - val_loss: 0.8280\n",
            "Epoch 14/30\n",
            "\u001b[1m619/619\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 18ms/step - accuracy: 0.7880 - loss: 0.4538 - val_accuracy: 0.4036 - val_loss: 1.5703\n",
            "Epoch 15/30\n",
            "\u001b[1m619/619\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 19ms/step - accuracy: 0.7781 - loss: 0.7303 - val_accuracy: 0.7422 - val_loss: 0.7333\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Model trained and saved at models/cnn_ecg_final.h5\n"
          ]
        }
      ],
      "source": [
        "# ================================================================\n",
        "# ðŸ¤– 6. Build and Train 1D-CNN for ECG Beat Classification\n",
        "# ================================================================\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# ------------------------------\n",
        "# ðŸ”¹ Load processed dataset\n",
        "# ------------------------------\n",
        "X = np.load(\"data/processed/beats.npy\")\n",
        "y = np.load(\"data/processed/labels.npy\")\n",
        "\n",
        "# Add channel dimension for Conv1D\n",
        "X = X[..., np.newaxis]\n",
        "\n",
        "# Train-validation split (stratified for balance)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, stratify=y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# One-hot encode labels\n",
        "num_classes = len(np.unique(y))\n",
        "y_train_cat = tf.keras.utils.to_categorical(y_train, num_classes=num_classes)\n",
        "y_val_cat = tf.keras.utils.to_categorical(y_val, num_classes=num_classes)\n",
        "\n",
        "# ------------------------------\n",
        "# âš–ï¸ Handle class imbalance\n",
        "# ------------------------------\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight=\"balanced\", classes=np.unique(y), y=y\n",
        ")\n",
        "cw_dict = {i: w for i, w in enumerate(class_weights)}\n",
        "print(\"âœ… Computed class weights:\", cw_dict)\n",
        "\n",
        "# ------------------------------\n",
        "# ðŸ—ï¸ Define 1D CNN model\n",
        "# ------------------------------\n",
        "def build_cnn(input_shape, num_classes):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Conv1D(32, kernel_size=7, padding=\"same\", activation=\"relu\", input_shape=input_shape),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.MaxPooling1D(2),\n",
        "\n",
        "        tf.keras.layers.Conv1D(64, kernel_size=5, padding=\"same\", activation=\"relu\"),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.MaxPooling1D(2),\n",
        "\n",
        "        tf.keras.layers.Conv1D(128, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.GlobalAveragePooling1D(),\n",
        "\n",
        "        tf.keras.layers.Dense(128, activation=\"relu\"),\n",
        "        tf.keras.layers.Dropout(0.4),\n",
        "        tf.keras.layers.Dense(num_classes, activation=\"softmax\")\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "model = build_cnn(X_train.shape[1:], num_classes)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "# ------------------------------\n",
        "# â³ Train model with callbacks\n",
        "# ------------------------------\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True),\n",
        "    tf.keras.callbacks.ModelCheckpoint(\"models/cnn_ecg_best.h5\", save_best_only=True)\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train_cat,\n",
        "    validation_data=(X_val, y_val_cat),\n",
        "    epochs=30,\n",
        "    batch_size=64,\n",
        "    class_weight=cw_dict,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# ------------------------------\n",
        "# ðŸ’¾ Save final model\n",
        "# ------------------------------\n",
        "model.save(\"models/cnn_ecg_final.h5\")\n",
        "print(\"âœ… Model trained and saved at models/cnn_ecg_final.h5\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rNF3O5dQdFs"
      },
      "source": [
        "3. Grad-CAM for Explainability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "WOcngMyyQftW"
      },
      "outputs": [],
      "source": [
        "# ================================================================\n",
        "# ðŸ” Grad-CAM for 1D CNN\n",
        "# ================================================================\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "def grad_cam_1d(model, signal, class_index, layer_name=\"last_conv\"):\n",
        "    \"\"\"\n",
        "    Generate Grad-CAM heatmap for 1D CNN input.\n",
        "    \n",
        "    Args:\n",
        "        model: Trained tf.keras model.\n",
        "        signal: 1D numpy array of shape (seq_len,) or (seq_len,1).\n",
        "        class_index: Target class index for explanation.\n",
        "        layer_name: Name of the last conv layer.\n",
        "    Returns:\n",
        "        heatmap (numpy array): Importance weights aligned to signal length.\n",
        "    \"\"\"\n",
        "    # Ensure correct shape: (1, seq_len, 1)\n",
        "    if signal.ndim == 1:\n",
        "        signal = np.expand_dims(signal, axis=-1)\n",
        "    signal = np.expand_dims(signal, axis=0)\n",
        "\n",
        "    # Create model mapping input -> (conv outputs, predictions)\n",
        "    conv_layer = model.get_layer(layer_name)\n",
        "    grad_model = tf.keras.models.Model(\n",
        "        inputs=model.inputs,\n",
        "        outputs=[conv_layer.output, model.output]\n",
        "    )\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        conv_outputs, predictions =_\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
