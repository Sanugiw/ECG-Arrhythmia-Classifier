{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsyhWgFjK8-Y"
      },
      "source": [
        "## **ECG Arrhythmia Classification with CNN and Interactive Dashboard**\n",
        "\n",
        "Electrocardiography (ECG) is a non-invasive technique that records the electrical activity of the heart over time. The ECG waveform reflects the coordinated depolarization and repolarization of cardiac muscle cells, mediated by the heart’s conduction system.\n",
        "\n",
        "### **Anatomical and Physiological Basis**\n",
        "\n",
        "The heart’s conduction system ensures rhythmic and synchronized contractions:\n",
        "\n",
        "* **Sinoatrial (SA) Node** – The natural pacemaker, located in the right atrium, initiates the electrical impulse.\n",
        "* **Atrial Muscle** – Conducts the impulse across both atria, producing the **P wave** (atrial depolarization).\n",
        "* **Atrioventricular (AV) Node** – Delays the impulse to allow ventricular filling, seen in the **PR segment**.\n",
        "* **Bundle of His & Bundle Branches** – Transmit the signal through the interventricular septum.\n",
        "* **Purkinje Fibers** – Rapidly deliver the impulse to ventricular myocardium, generating the **QRS complex** (ventricular depolarization) followed by the **T wave** (ventricular repolarization).\n",
        "\n",
        "### **Arrhythmias and ECG Changes**\n",
        "\n",
        "Arrhythmias occur when the impulse generation or conduction pathway is altered:\n",
        "\n",
        "* **Normal beat (N)** – Regular SA node rhythm with intact conduction.\n",
        "* **Ventricular ectopic beat (VEB)** – Premature ventricular depolarization from an abnormal focus in the ventricles, often producing a wide QRS complex.\n",
        "* **Supraventricular ectopic beat (SVEB)** – Originates above the ventricles (atria or AV node) and alters P-wave morphology with a narrow QRS.\n",
        "* **Fusion beat (F)** – A hybrid waveform from simultaneous normal and ectopic activation.\n",
        "\n",
        "These morphological differences are directly tied to the anatomical site of origin, making ECG classification both clinically relevant and physiologically interpretable.\n",
        "\n",
        "### **Project Objective**\n",
        "\n",
        "In this project, we develop a **Convolutional Neural Network (CNN)** model to classify ECG beats into different arrhythmia types using the MIT-BIH Arrhythmia Database. The model automatically learns morphological features such as P-wave shape, QRS width, and ST-T segment variations that correspond to underlying conduction abnormalities.\n",
        "\n",
        "To make the results accessible and interpretable, we integrate the trained model into a **Streamlit-based interactive dashboard** that allows users to:\n",
        "\n",
        "* Upload ECG files or explore sample beats\n",
        "* View the raw waveform and detected beats\n",
        "* See classification results with confidence scores\n",
        "* Explore explainability visualizations (e.g., Grad-CAM) mapping model attention to specific waveform regions\n",
        "* Connect waveform changes to anatomical and physiological causes\n",
        "\n",
        "This combination of deep learning, interactive visualization, and anatomical context bridges the gap between machine intelligence and clinical reasoning.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_jFOesPPf2I",
        "outputId": "cb27efab-a917-4253-f488-34212a0b7aaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Folder structure ready.\n",
            "Generating record list for: 100\n",
            "Generating list of all files for: 100\n",
            "Downloading files...\n",
            "Finished downloading files\n",
            "Generating record list for: 101\n",
            "Generating list of all files for: 101\n",
            "Downloading files...\n",
            "Finished downloading files\n",
            "Generating record list for: 102\n",
            "Generating list of all files for: 102\n",
            "Downloading files...\n",
            "Finished downloading files\n",
            "Generating record list for: 103\n",
            "Generating list of all files for: 103\n",
            "Downloading files...\n",
            "Finished downloading files\n",
            "Generating record list for: 104\n",
            "Generating list of all files for: 104\n",
            "Downloading files...\n",
            "Finished downloading files\n",
            "✅ MIT-BIH sample records downloaded.\n",
            "Extracted beats: (2270, 216), Sampling rate: 360 Hz\n"
          ]
        }
      ],
      "source": [
        "# ================================================================\n",
        "# 📦 1. Install dependencies\n",
        "# ================================================================\n",
        "!pip install wfdb streamlit plotly tensorflow scikit-learn neurokit2 tf-explain -q\n",
        "\n",
        "# ================================================================\n",
        "# 📂 2. Create folder structure\n",
        "# ================================================================\n",
        "import os\n",
        "\n",
        "folders = [\n",
        "    \"data/raw\",\n",
        "    \"data/processed\",\n",
        "    \"models\",\n",
        "    \"src/data\",\n",
        "    \"src/models\",\n",
        "    \"app\"\n",
        "]\n",
        "for f in folders:\n",
        "    os.makedirs(f, exist_ok=True)\n",
        "\n",
        "print(\"✅ Folder structure ready.\")\n",
        "\n",
        "# ================================================================\n",
        "# 📥 3. Download MIT-BIH Arrhythmia Database (records 100-104 for demo)\n",
        "# ================================================================\n",
        "import wfdb\n",
        "\n",
        "record_ids = [\"100\", \"101\", \"102\", \"103\", \"104\"]\n",
        "for rec in record_ids:\n",
        "    wfdb.dl_database(\n",
        "        \"mitdb\",\n",
        "        os.path.join(\"data/raw\", rec),\n",
        "        records=[rec]\n",
        "    )\n",
        "print(\"✅ MIT-BIH sample records downloaded.\")\n",
        "\n",
        "# ================================================================\n",
        "# 🛠 4. Preprocessing functions\n",
        "# ================================================================\n",
        "import numpy as np\n",
        "import neurokit2 as nk\n",
        "from scipy.signal import butter, filtfilt\n",
        "\n",
        "def bandpass_filter(signal, fs, lowcut=0.5, highcut=40.0, order=4):\n",
        "    \"\"\"Bandpass filter for ECG signal.\"\"\"\n",
        "    nyq = 0.5 * fs\n",
        "    low = lowcut / nyq\n",
        "    high = highcut / nyq\n",
        "    b, a = butter(order, [low, high], btype='band')\n",
        "    return filtfilt(b, a, signal)\n",
        "\n",
        "def preprocess_ecg(record_path):\n",
        "    \"\"\"Load, filter, detect R-peaks, and segment beats.\"\"\"\n",
        "    # Load ECG\n",
        "    rec = wfdb.rdrecord(record_path)\n",
        "    sig = rec.p_signal[:, 0]  # first channel\n",
        "    fs = rec.fs\n",
        "\n",
        "    # Filter\n",
        "    sig_filtered = bandpass_filter(sig, fs)\n",
        "\n",
        "    # R-peak detection\n",
        "    _, rpeaks = nk.ecg_peaks(sig_filtered, sampling_rate=fs)\n",
        "\n",
        "    # Segment beats\n",
        "    beats = []\n",
        "    window_pre = int(0.2 * fs)   # 200 ms before R-peak\n",
        "    window_post = int(0.4 * fs)  # 400 ms after R-peak\n",
        "    for r in rpeaks['ECG_R_Peaks']:\n",
        "        start = r - window_pre\n",
        "        end = r + window_post\n",
        "        if start >= 0 and end < len(sig_filtered):\n",
        "            beats.append(sig_filtered[start:end])\n",
        "    beats = np.array(beats)\n",
        "\n",
        "    return beats, sig_filtered, fs, rpeaks['ECG_R_Peaks']\n",
        "\n",
        "# Test with one record\n",
        "beats, sig_filt, fs, rlocs = preprocess_ecg(\"data/raw/100/100\")\n",
        "print(f\"Extracted beats: {beats.shape}, Sampling rate: {fs} Hz\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0szKtOQQQrf"
      },
      "source": [
        "1. Data Preparation and Labelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTZIMTrJQOvo",
        "outputId": "5ff108ca-9464-4a28-eb82-3fcfb5c2506f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Saved 9913 beats. Shape per beat: 216 samples\n"
          ]
        }
      ],
      "source": [
        "# ================================================================\n",
        "# 📥 5. Beat extraction with labels\n",
        "# ================================================================\n",
        "import wfdb\n",
        "from collections import defaultdict\n",
        "\n",
        "# Mapping from MIT-BIH annotation symbols to simplified class labels\n",
        "# N = Normal, V = Ventricular ectopic, S = Supraventricular ectopic, F = Fusion, Q = Unknown\n",
        "symbol_to_class = {\n",
        "    'N': 0,  # Normal\n",
        "    'L': 0, 'R': 0, 'e': 0, 'j': 0,\n",
        "    'V': 1, 'E': 1,\n",
        "    'S': 2, 'A': 2, 'a': 2, 'J': 2,\n",
        "    'F': 3,\n",
        "    '/': 4, 'Q': 4, '?': 4\n",
        "}\n",
        "\n",
        "class_names = [\"Normal\", \"VEB\", \"SVEB\", \"Fusion\", \"Unknown\"]\n",
        "\n",
        "def extract_beats_with_labels(record_path):\n",
        "    # Load ECG signal and annotations\n",
        "    rec = wfdb.rdrecord(record_path)\n",
        "    ann = wfdb.rdann(record_path, 'atr')\n",
        "    sig = rec.p_signal[:, 0]\n",
        "    fs = rec.fs\n",
        "\n",
        "    # Filter signal\n",
        "    sig_filtered = bandpass_filter(sig, fs)\n",
        "\n",
        "    beats, labels = [], []\n",
        "    window_pre = int(0.2 * fs)\n",
        "    window_post = int(0.4 * fs)\n",
        "\n",
        "    for idx, sym in zip(ann.sample, ann.symbol):\n",
        "        if sym in symbol_to_class:\n",
        "            start = idx - window_pre\n",
        "            end = idx + window_post\n",
        "            if start >= 0 and end < len(sig_filtered):\n",
        "                beats.append(sig_filtered[start:end])\n",
        "                labels.append(symbol_to_class[sym])\n",
        "\n",
        "    return np.array(beats), np.array(labels)\n",
        "\n",
        "# Process all demo records\n",
        "all_beats, all_labels = [], []\n",
        "for rec in record_ids:\n",
        "    beats, labels = extract_beats_with_labels(f\"data/raw/{rec}/{rec}\")\n",
        "    all_beats.append(beats)\n",
        "    all_labels.append(labels)\n",
        "\n",
        "all_beats = np.vstack(all_beats)\n",
        "all_labels = np.concatenate(all_labels)\n",
        "\n",
        "# Save processed dataset\n",
        "np.save(\"data/processed/beats.npy\", all_beats)\n",
        "np.save(\"data/processed/labels.npy\", all_labels)\n",
        "\n",
        "print(f\"✅ Saved {all_beats.shape[0]} beats. Shape per beat: {all_beats.shape[1]} samples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWEA-FFLQXZt"
      },
      "source": [
        "CNN Model Definition & Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eEu0jYLbQaWo",
        "outputId": "0b54ef52-111c-4f7a-afce-8b598014c95f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 134ms/step - accuracy: 0.6720 - loss: 1.8247 - val_accuracy: 0.3459 - val_loss: 2.2321\n",
            "Epoch 2/20\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 70ms/step - accuracy: 0.7723 - loss: 0.4849 - val_accuracy: 0.3459 - val_loss: 5.2172\n",
            "Epoch 3/20\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 59ms/step - accuracy: 0.8374 - loss: 0.2750 - val_accuracy: 0.3459 - val_loss: 5.3485\n",
            "Epoch 4/20\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 59ms/step - accuracy: 0.8152 - loss: 0.2791 - val_accuracy: 0.3485 - val_loss: 3.5498\n",
            "Epoch 5/20\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 69ms/step - accuracy: 0.8660 - loss: 0.3257 - val_accuracy: 0.5618 - val_loss: 0.9041\n",
            "Epoch 6/20\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 66ms/step - accuracy: 0.8633 - loss: 0.1922 - val_accuracy: 0.8008 - val_loss: 0.4817\n",
            "Epoch 7/20\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - accuracy: 0.9092 - loss: 0.1440 - val_accuracy: 0.8245 - val_loss: 0.3388\n",
            "Epoch 8/20\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 66ms/step - accuracy: 0.9066 - loss: 0.1376 - val_accuracy: 0.7791 - val_loss: 0.5691\n",
            "Epoch 9/20\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 66ms/step - accuracy: 0.8394 - loss: 0.2911 - val_accuracy: 0.6480 - val_loss: 1.7161\n",
            "Epoch 10/20\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 57ms/step - accuracy: 0.8296 - loss: 0.4057 - val_accuracy: 0.8921 - val_loss: 0.3464\n",
            "Epoch 11/20\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 63ms/step - accuracy: 0.8687 - loss: 0.2268 - val_accuracy: 0.8764 - val_loss: 0.2491\n",
            "Epoch 12/20\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 66ms/step - accuracy: 0.8631 - loss: 0.1775 - val_accuracy: 0.9123 - val_loss: 0.1886\n",
            "Epoch 13/20\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 76ms/step - accuracy: 0.8785 - loss: 0.1949 - val_accuracy: 0.9854 - val_loss: 0.0901\n",
            "Epoch 14/20\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 67ms/step - accuracy: 0.9344 - loss: 0.1346 - val_accuracy: 0.8830 - val_loss: 0.2332\n",
            "Epoch 15/20\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 59ms/step - accuracy: 0.8675 - loss: 0.1966 - val_accuracy: 0.9859 - val_loss: 0.0887\n",
            "Epoch 16/20\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 68ms/step - accuracy: 0.9008 - loss: 0.1810 - val_accuracy: 0.8785 - val_loss: 0.2524\n",
            "Epoch 17/20\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 56ms/step - accuracy: 0.9295 - loss: 0.1184 - val_accuracy: 0.8230 - val_loss: 0.4256\n",
            "Epoch 18/20\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 65ms/step - accuracy: 0.9304 - loss: 0.0997 - val_accuracy: 0.6768 - val_loss: 0.8095\n",
            "Epoch 19/20\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - accuracy: 0.8749 - loss: 0.2282 - val_accuracy: 0.8880 - val_loss: 0.2471\n",
            "Epoch 20/20\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 56ms/step - accuracy: 0.8970 - loss: 0.1373 - val_accuracy: 0.9213 - val_loss: 0.1519\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Model trained and saved.\n"
          ]
        }
      ],
      "source": [
        "# ================================================================\n",
        "# 🤖 6. Build and train 1D-CNN\n",
        "# ================================================================\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Load data\n",
        "X = np.load(\"data/processed/beats.npy\")\n",
        "y = np.load(\"data/processed/labels.npy\")\n",
        "\n",
        "# Add channel dimension\n",
        "X = X[..., np.newaxis]\n",
        "\n",
        "# Train/val split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
        "\n",
        "# One-hot encode labels\n",
        "y_train_cat = tf.keras.utils.to_categorical(y_train, num_classes=len(class_names))\n",
        "y_val_cat = tf.keras.utils.to_categorical(y_val, num_classes=len(class_names))\n",
        "\n",
        "# Compute class weights for imbalance\n",
        "class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y), y=y)\n",
        "cw_dict = {i: w for i, w in enumerate(class_weights)}\n",
        "\n",
        "# CNN model\n",
        "def build_cnn(input_shape, num_classes):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Conv1D(32, kernel_size=7, padding=\"same\", activation=\"relu\", input_shape=input_shape),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.MaxPooling1D(2),\n",
        "\n",
        "        tf.keras.layers.Conv1D(64, kernel_size=5, padding=\"same\", activation=\"relu\"),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.MaxPooling1D(2),\n",
        "\n",
        "        tf.keras.layers.Conv1D(128, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.GlobalAveragePooling1D(),\n",
        "\n",
        "        tf.keras.layers.Dense(128, activation=\"relu\"),\n",
        "        tf.keras.layers.Dropout(0.4),\n",
        "        tf.keras.layers.Dense(num_classes, activation=\"softmax\")\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "model = build_cnn(X_train.shape[1:], len(class_names))\n",
        "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Train\n",
        "history = model.fit(\n",
        "    X_train, y_train_cat,\n",
        "    validation_data=(X_val, y_val_cat),\n",
        "    epochs=20,\n",
        "    batch_size=64,\n",
        "    class_weight=cw_dict\n",
        ")\n",
        "\n",
        "# Save model\n",
        "model.save(\"models/cnn_ecg.h5\")\n",
        "print(\"✅ Model trained and saved.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rNF3O5dQdFs"
      },
      "source": [
        "3. Grad-CAM for Explainability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOcngMyyQftW"
      },
      "outputs": [],
      "source": [
        "# ================================================================\n",
        "# 🔍 7. Grad-CAM implementation for 1D CNN\n",
        "# ================================================================\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "def grad_cam_1d(model, signal, class_index):\n",
        "    grad_model = tf.keras.models.Model([model.inputs], [model.get_layer(index=-3).output, model.output])\n",
        "    with tf.GradientTape() as tape:\n",
        "        conv_outputs, predictions = grad_model(tf.expand_dims(signal, axis=0))\n",
        "        loss = predictions[:, class_index]\n",
        "    grads = tape.gradient(loss, conv_outputs)\n",
        "    pooled_grads = tf.reduce_mean(grads, axis=1)\n",
        "    conv_outputs = conv_outputs.numpy()[0]\n",
        "    pooled_grads = pooled_grads.numpy()\n",
        "\n",
        "    for i in range(conv_outputs.shape[-1]):\n",
        "        conv_outputs[:, i] *= pooled_grads[i]\n",
        "    heatmap = np.mean(conv_outputs, axis=-1)\n",
        "    heatmap = np.maximum(heatmap, 0)\n",
        "    heatmap /= np.max(heatmap) if np.max(heatmap) != 0 else 1\n",
        "    return heatmap\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
